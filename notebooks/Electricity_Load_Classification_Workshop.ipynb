{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "# Table of Contents\n",
    " <p><div class=\"lev1 toc-item\"><a href=\"#Import-data\" data-toc-modified-id=\"Import-data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Import data</a></div><div class=\"lev2 toc-item\"><a href=\"#If-you-have-got-the-data-stored-locally-on-your-computer\" data-toc-modified-id=\"If-you-have-got-the-data-stored-locally-on-your-computer-11\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>If you have got the data stored locally on your computer</a></div><div class=\"lev2 toc-item\"><a href=\"#If-working-with-colab\" data-toc-modified-id=\"If-working-with-colab-12\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>If working with colab</a></div><div class=\"lev1 toc-item\"><a href=\"#Exploratory-Data-Analysis\" data-toc-modified-id=\"Exploratory-Data-Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory Data Analysis</a></div><div class=\"lev2 toc-item\"><a href=\"#Check-data-index\" data-toc-modified-id=\"Check-data-index-21\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Check data index</a></div><div class=\"lev2 toc-item\"><a href=\"#Missing/null-values\" data-toc-modified-id=\"Missing/null-values-22\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Missing/null values</a></div><div class=\"lev2 toc-item\"><a href=\"#Visualise-some-sequences\" data-toc-modified-id=\"Visualise-some-sequences-23\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Visualise some sequences</a></div><div class=\"lev2 toc-item\"><a href=\"#Seasonality---do-we-have-any?\" data-toc-modified-id=\"Seasonality---do-we-have-any?-24\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Seasonality - do we have any?</a></div><div class=\"lev2 toc-item\"><a href=\"#Group-users-by-usage?\" data-toc-modified-id=\"Group-users-by-usage?-25\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Group users by usage?</a></div><div class=\"lev2 toc-item\"><a href=\"#Downsample-for-better-visualisation\" data-toc-modified-id=\"Downsample-for-better-visualisation-26\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>Downsample for better visualisation</a></div><div class=\"lev2 toc-item\"><a href=\"#Other-things-that-can-be-done-during-preprocessing\" data-toc-modified-id=\"Other-things-that-can-be-done-during-preprocessing-27\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>Other things that can be done during preprocessing</a></div><div class=\"lev1 toc-item\"><a href=\"#Preparation-for-Classification\" data-toc-modified-id=\"Preparation-for-Classification-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Preparation for Classification</a></div><div class=\"lev2 toc-item\"><a href=\"#Load-labels\" data-toc-modified-id=\"Load-labels-31\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Load labels</a></div><div class=\"lev1 toc-item\"><a href=\"#Random-forest\" data-toc-modified-id=\"Random-forest-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Random forest</a></div><div class=\"lev2 toc-item\"><a href=\"#Create-features\" data-toc-modified-id=\"Create-features-41\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Create features</a></div><div class=\"lev3 toc-item\"><a href=\"#Split-into-train-and-test-set\" data-toc-modified-id=\"Split-into-train-and-test-set-411\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;</span>Split into train and test set</a></div><div class=\"lev3 toc-item\"><a href=\"#Classify\" data-toc-modified-id=\"Classify-412\"><span class=\"toc-item-num\">4.1.2&nbsp;&nbsp;</span>Classify</a></div><div class=\"lev3 toc-item\"><a href=\"#Assess-our-results\" data-toc-modified-id=\"Assess-our-results-413\"><span class=\"toc-item-num\">4.1.3&nbsp;&nbsp;</span>Assess our results</a></div><div class=\"lev2 toc-item\"><a href=\"#As-a-comparison-with-a-meaningless-dataset\" data-toc-modified-id=\"As-a-comparison-with-a-meaningless-dataset-42\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>As a comparison with a meaningless dataset</a></div><div class=\"lev1 toc-item\"><a href=\"#Neural-network-solution\" data-toc-modified-id=\"Neural-network-solution-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Neural network solution</a></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: \n",
    "https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014#\n",
    "\n",
    "**Abstract:** This data set contains electricity consumption of 370 points/clients.\n",
    "\n",
    "**Data Set Information:**\n",
    "\n",
    "Data set has no missing values.    \n",
    "Values are in kW of each 15 min. To convert values in kWh values must be divided by 4.     \n",
    "Each column represent one client. Some clients were created after 2011. In these cases consumption were considered zero.     \n",
    "All time labels report to Portuguese hour. However all days present 96 measures (24*4). Every year in March time change day (which has only 23 hours) the values between 1:00 am and 2:00 am are zero for all points. Every year in October time change day (which has 25 hours) the values between 1:00 am and 2:00 am aggregate the consumption of two hours. \n",
    "\n",
    "\n",
    "**Attribute Information:**\n",
    "* Data set were saved as txt using csv format, using semi colon (;). \n",
    "* First column present date and time as a string with the following format 'yyyy-mm-dd hh:mm:ss' \n",
    "* Other columns present float values with consumption in kW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have got the data stored locally on your computer\n",
    "Otherwise see further below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = '' # fill in correct data path here\n",
    "df = pd.read_csv(path + '/ElectricityLoad_Workshop.csv',index_col=0,parse_dates=[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If working with colab\n",
    "You will need to download the data from the github repository, and upzip it.   \n",
    "Colab can mount your Google Drive, so you will need to copy the data file you want to work with onto your Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount you Google Drive --> you will need to click on the link that will come up \n",
    "# and type in the authentication code that will be generated\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now read in data\n",
    "path = '/content/drive/My Drive/your_directory_for_this_workshop' # adjust the path as needed\n",
    "df = pd.read_csv(path + '/ElectricityLoad_Workshop.csv',index_col=0,parse_dates=[0])\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The data runs from', min(df.index), 'to', max(df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many data measurements do we have?\n",
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many columns = user entries?\n",
    "df.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data index\n",
    "Do we have consecutive dates, or are some dates missing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: look at datetimes\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: compute difference between each item and the one after --> creates an array of nanosecond differences\n",
    "np.diff(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3: how many unique differences do we have in that array?\n",
    "# ideally one one, i.e. all rows are 'equidistant'\n",
    "np.unique(np.diff(df.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing/null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all values\n",
    "list(df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# too long to read\n",
    "np.unique(df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise some sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first item\n",
    "df.iloc[:,0].plot(figsize=(15,10),marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last item\n",
    "df.iloc[:,-1].plot(figsize=(15,10),marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on a subset of time - one year\n",
    "start_date = pd.Timestamp('2012-01-01 00:00:00')\n",
    "end_date = pd.Timestamp('2012-12-31 00:00:00')\n",
    "\n",
    "df_subset = df[start_date:end_date]\n",
    "df_subset.iloc[:,100].plot(figsize=(15,10),marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on a subset of time - one day\n",
    "start_date = pd.Timestamp('2012-01-01 00:00:00')\n",
    "end_date = pd.Timestamp('2012-01-02 00:00:00')\n",
    "\n",
    "df_subset = df[start_date:end_date]\n",
    "df_subset.iloc[:,100].plot(figsize=(15,10),marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoom in on a subset of time : one day\n",
    "start_date = pd.Timestamp('2012-01-02 00:00:00')\n",
    "end_date = pd.Timestamp('2012-01-03 00:00:00')\n",
    "\n",
    "df_subset = df[start_date:end_date]\n",
    "df_subset.iloc[:,100].plot(figsize=(15,10),marker='*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seasonality - do we have any?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/\n",
    "\n",
    "A given time series is thought to consist of three systematic components including **level, trend, seasonality**, and one non-systematic component called **noise**.\n",
    "\n",
    "These components are defined as follows:\n",
    "* Level: The average value in the series.\n",
    "* Trend: The increasing or decreasing value in the series.\n",
    "* Seasonality: The repeating short-term cycle in the series.\n",
    "* Noise: The random variation in the series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facebook's prophet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = df.iloc[:,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series.plot(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's read the documentation for this command\n",
    "seasonal_decompose?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additive seasonality?\n",
    "result = seasonal_decompose(series, model='additive',freq=1)\n",
    "#print(result.trend)\n",
    "#print(result.seasonal)\n",
    "#print(result.resid)\n",
    "#print(result.observed)\n",
    "result.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no additive seasonality in this time series example, and nothing attributed to random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# multiplicative seasonality?\n",
    "\n",
    "# multiplicative seasonality canot handle values of 0\n",
    "result = seasonal_decompose(series+0.001, model='multiplicative',freq=1)\n",
    "result.plot()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be no multiplicative seasonality in this time series example, and nothing attributed to random noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "NOTE: There are other, (perhaps more sophisticated ways) to detect and deal with seasonality --> search online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group users by usage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sum(axis=0).hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see several groups of usage.   \n",
    "There moght be a group with overall usage <= 100, and one above, but not clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsample for better visualisation\n",
    "\n",
    "Some info on date offset: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_hourly = df.resample(axis=0,rule='H').mean()\n",
    "# df_hourly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_daily = df.resample(axis=0,rule='1d').mean()\n",
    "# df_daily.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_weekly = df.resample(axis=0,rule='W').mean()\n",
    "df_weekly.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weekly.plot(figsize=(15,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other things that can be done during preprocessing\n",
    "\n",
    "* standardise data (make mean == 0 and standard deviation ==1)\n",
    "* re-scale data (change values to be between a given min and max value)\n",
    "* check on null values and forward fill (ffill) or backward fill, depending on context\n",
    "* if you have any missing rows, i.e. time stamps, then create to make equidistant time steps\n",
    "* apply some signal processing, if that might be applicable, for instance, low-band filter\n",
    "* detect and remove data items in the signal that are not relevant, for insance, when a machine was out of order\n",
    "* detect outliers and investigate / correct / drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are implementations for all kinds of pre-processing tasks within the pandas and scikit-learn packages. See documentation and tutorials on   \n",
    "https://scikit-learn.org/stable/documentation.html   \n",
    "and   \n",
    "https://pandas.pydata.org/pandas-docs/stable/#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading from local directory\n",
    "path = '' # fill in correct data path here\n",
    "labels_df = pd.read_csv(path + '/ElectricityLoad_Workshop_Labels.csv',index_col=[0])\n",
    "\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if loading from google drive\n",
    "path = '/content/drive/My Drive/your_directory_for_this_workshop' # adjust the path as needed\n",
    "df = pd.read_csv(path + '/ElectricityLoad_Workshop.csv',index_col=0,parse_dates=[0])\n",
    "\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many different labels do we have?\n",
    "labels_df.labels.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html   \n",
    "\n",
    "'A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.'\n",
    "\n",
    "Input has got to be one data entry == one array of feature values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features\n",
    "\n",
    "We will use each column of the data, i.e. each individual user, as an input data entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need column-wise values as arrays\n",
    "X = []\n",
    "\n",
    "for one_col in df.columns:\n",
    "    X.append(df.iloc[:][one_col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = labels_df.labels.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(y_train, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_elements, counts_elements = np.unique(y_test, return_counts=True)\n",
    "print(\"Frequency of unique values of the said array:\")\n",
    "print(np.asarray((unique_elements, counts_elements)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classifier instance\n",
    "clf_full_timeseries = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit our training data\n",
    "clf_full_timeseries.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions = clf_full_timeseries.predict(X_test)\n",
    "print(y_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_predictions, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** is the answer to 'What proportion of items do we assign to the correct class?'  \n",
    "The next three measures help to interpret the results with focus on one class ('Positive class') vs. the other class or classes ('Negative class').   \n",
    "**Precision** answers the question 'What proportion of items predicted to be of Positive class are actually Positives?'   \n",
    "**Recall** is the answer to 'What proportion of Postive items has been correctly assigned to the Positive class?'   \n",
    "**F1-score** is the harmonic mean of Precision and Recall, calculated by 2* ((precision * recall)/(precision + recall))   \n",
    "Read more on https://en.wikipedia.org/wiki/F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Discussion: this went surprisingly well! (We don't expect 100% performance; that is actually something to be sceptical about)   \n",
    "Can we investigate what the features were that the classifier used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clf_full_timeseries.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(clf_full_timeseries.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(clf_full_timeseries.feature_importances_ != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(clf_full_timeseries.feature_importances_ != 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf_full_timeseries.feature_importances_[np.where(clf_full_timeseries.feature_importances_ != 0)[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As a comparison with a meaningless dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + '/ElectricityLoad_Workshop_NonMeaningfulDataset.csv',index_col=0,parse_dates=[0])\n",
    "\n",
    "# we need column-wise values as arrays\n",
    "X = []\n",
    "\n",
    "for one_col in df.columns:\n",
    "    X.append(df.iloc[:][one_col].values)\n",
    "    \n",
    "y = labels_df.labels.values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# create a classifier instance\n",
    "clf_full_timeseries = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# fit our training data\n",
    "clf_full_timeseries.fit(X_train, y_train)  \n",
    "\n",
    "y_predictions = clf_full_timeseries.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_predictions, target_names=['class 0', 'class 1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "The modelling and classification approach with a dataset that does not have any inherent patterns shows results that are around 50% accuracy. Which means in 50% of cases we assign the correct class, which is equivalent to 'by chance'.   \n",
    "This is a result to be expected for a dataset without actual patterns in data. (Unless one chose an unsuitable modelling approach or feature selection...)      \n",
    "\n",
    "Compared to the results with our previous dataset, we can conclude that the same approach as in the last setup, but with different data, did find patterns in the data. The excellent performance is likely due to the fact that the dataset was 'crafted' for the tutorial in a way that had stark differences within the two classes' data ranges, which was picked up very well by the random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network solution\n",
    "\n",
    "We will not have time to cover solution approaches for time series data with neaural network architectures.\n",
    "\n",
    "There are some very good step-by-step tutorials out there for this, take a look at:\n",
    "* 'How to Use the TimeseriesGenerator for Time Series Forecasting in Keras' https://machinelearningmastery.com/how-to-use-the-timeseriesgenerator-for-time-series-forecasting-in-keras/\n",
    "* Deep Learning for time series tutorials https://machinelearningmastery.com/category/deep-learning-time-series/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "340.4347839355469px",
    "width": "251.7391357421875px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": false,
   "threshold": 4,
   "toc_cell": true,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
